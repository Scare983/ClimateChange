{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of our project is to determine the correction between temperature and greenhouse emmissions based on data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import getopt\n",
    "import sys\n",
    "import urllib.request\n",
    "from os.path import exists\n",
    "from os import remove\n",
    "import re\n",
    "import numpy as np\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import glob\n",
    "from calendar import monthrange\n",
    "import datetime as dt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first did data collection to retrieve data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "argumentHash = {'ch4':False, 'n02':True, 'weather': False, 'sf6':False, 'debug':False}\n",
    "def prossargs():\n",
    "    try:\n",
    "        opt, args = getopt.getopt(sys.argv[1:], \"cwnshd\")\n",
    "    except getopt.GetoptError:\n",
    "        usage()\n",
    "    for opts, arg in opt:\n",
    "        if opts == '-h':\n",
    "            help()\n",
    "        elif opts == '-c':\n",
    "            argumentHash['ch4'] = True\n",
    "        elif opts == 'n':\n",
    "            arg['n02'] = True\n",
    "        elif opts == '-w':\n",
    "            argumentHash['weather'] = True\n",
    "        elif opts == '-s':\n",
    "            argumentHash['sf6'] = True\n",
    "        elif opts == '-d':\n",
    "            argumentHash['debug'] = True\n",
    "        else:\n",
    "            help()\n",
    "    flag = False\n",
    "    for a in argumentHash.keys():\n",
    "        if argumentHash[a]:\n",
    "            flag = False\n",
    "            break\n",
    "        else:\n",
    "\n",
    "            flag = True\n",
    "    if flag:\n",
    "        print('No arguments turned on.')\n",
    "        help()\n",
    "\n",
    "\n",
    "def help():\n",
    "    print('This file is used to update or retrieve infromation from websites ')\n",
    "    print('the data found is stored inside a ./data folder and goes to corresponding ')\n",
    "    print('folders for the desired atmospheric condition we are grabbing.')\n",
    "    usage()\n",
    "\n",
    "def usage():\n",
    "    print('please input correct flags to update data files ')\n",
    "    print('c = CH4, w = weather, n=n02 s=sf6 h=help d=debugMode')\n",
    "    print(\n",
    "        \"USAGE:  python retrieveData.py [-cwnshd]\")\n",
    "    exit(2)\n",
    "\n",
    "def debug(statement):\n",
    "    if argumentHash['debug']:\n",
    "        print(statement)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def getCh4():\n",
    "    # this website has several pages of .txt files that we have to parse and\n",
    "    # grab the data through ftp,\n",
    "    # different webpages can be associted with pageID=###\n",
    "    website = 'https://www.esrl.noaa.gov/gmd/dv/data/index.php?pageID=1&category=Greenhouse%2BGases&parameter_name=Methane'\n",
    "    page = requests.get(website)\n",
    "    debug(page.status_code)\n",
    "    i = 1\n",
    "\n",
    "    while page.status_code == 200:\n",
    "        base = './data/Ch4/'\n",
    "        soup = BeautifulSoup(page.content, 'html.parser')\n",
    "        tbody = soup.find('tbody')\n",
    "        #debug(tbody.find_all('tr'))\n",
    "        # get first span in first td. which should be ID.\n",
    "        currIndex = tbody.find('tr').td.span.get_text()\n",
    "        #debug(currIndex)\n",
    "\n",
    "        for tr in tbody.find_all('tr'):\n",
    "            debug(tr.find_all('td')[1].span.get_text())\n",
    "            allTD = tr.find_all('td')\n",
    "            cityCountry = allTD[1].span.get_text()\n",
    "            debug(allTD[5].get_text())\n",
    "            isMonth = allTD[5].get_text()\n",
    "            # debug(allTD[7].a['href'])\n",
    "            #we want to store the data if this is true\n",
    "            data = urllib.request.Request(allTD[7].a['href'])\n",
    "\n",
    "\n",
    "            # actual\n",
    "            if isMonth == 'Monthly Averages':\n",
    "                data = urllib.request.Request(allTD[7].a['href'])\n",
    "                with urllib.request.urlopen(data) as response:\n",
    "                    downloadPage = response.read()\n",
    "                nameList = cityCountry.split(',')\n",
    "                fileNameTxt = base + \"_\".join(nameList)\n",
    "                fileNameCsv = base + \"_\".join(nameList) + '.csv'\n",
    "                if exists(fileNameCsv):\n",
    "\n",
    "                    # might want to change this to break out of all loops\n",
    "                    break\n",
    "                else:\n",
    "\n",
    "                    downloadPage = downloadPage.decode()\n",
    "\n",
    "                    fileHandle = open(fileNameTxt, 'w')\n",
    "                    fileHandle.write(downloadPage)\n",
    "                    fileHandle.close()\n",
    "                    fileHandle = open(fileNameTxt, 'r')\n",
    "                    line = fileHandle.readline()\n",
    "                    # remove comments. But we need last comment\n",
    "                    dataHeader = \"\"\n",
    "                    fileHandle2 = open(fileNameCsv, 'w')\n",
    "                    while line:\n",
    "                        if 'data_fields:' in line:\n",
    "                            dataHeader = line.replace('# data_fields: ', \"\")\n",
    "                            # debug(dataHeader)\n",
    "                            dataHeader = dataHeader.replace(' ', ',')\n",
    "                            fileHandle2.write(dataHeader)\n",
    "                        else:\n",
    "                            newLine = re.sub(r'#.*', '', line)\n",
    "                            newLine = re.sub(r' +', ' ', newLine)\n",
    "                            if newLine != \"\\n\":\n",
    "                                # print(newLine)\n",
    "                                newLine = newLine.split(' ')\n",
    "                                newLine = ','.join(newLine)\n",
    "                                fileHandle2.write(newLine)\n",
    "                        line = fileHandle.readline()\n",
    "                    fileHandle.close()\n",
    "                    fileHandle2.close()\n",
    "                    remove(fileNameTxt)\n",
    "        i+=1\n",
    "        website ='https://www.esrl.noaa.gov/gmd/dv/data/index.php?pageID={}&category=Greenhouse%2BGases&parameter_name=Methane'.format(i)\n",
    "        page = requests.get(website)\n",
    "        if page.status_code == 200:\n",
    "        # check index with previous\n",
    "            soup = BeautifulSoup(page.content, 'html.parser')\n",
    "            tbody = soup.find('tbody')\n",
    "            nextIndex = tbody.find('tr').td.span.get_text()\n",
    "            if nextIndex == currIndex:\n",
    "            #            # there are no more pages and we go into a loop if we do not do this.\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "        else:\n",
    "            break\n",
    "\n",
    "def updateCh4():\n",
    "    pass\n",
    "\n",
    "\n",
    "def getWeather():\n",
    "    pass\n",
    "\n",
    "def updateWeather():\n",
    "    pass\n",
    "\n",
    "def getN02():\n",
    "    pass\n",
    "\n",
    "def updateN02():\n",
    "    pass\n",
    "\n",
    "def getSf6():\n",
    "    pass\n",
    "\n",
    "def updateSf6():\n",
    "    pass\n",
    "\n",
    "### main ###\n",
    "prossargs()\n",
    "if argumentHash['ch4']:\n",
    "    getCh4()\n",
    "if argumentHash['weather']:\n",
    "    getWeather()  # This needs to be implemented\n",
    "if argumentHash['sf6']:\n",
    "    getSf6() # this one file that is found and downloaded.\n",
    "if argumentHash['n02']:\n",
    "    # this is one file, found and downloaded to folder\n",
    "    getN02()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DateMod():\n",
    "    yearDataFrame = None\n",
    "    monthDataFrame = None\n",
    "    weekDataFrame = None\n",
    "    dayDataFrame = None\n",
    "    def __init__(self, df, colValName):\n",
    "        if \"month\" in df.keys():\n",
    "            self.month_to_days(df,colValName)\n",
    "        elif 'year' in df.keys():\n",
    "            self.year_to_days(df,colValName)\n",
    "        #TODO: fill in nan values created from months_to_day (They will = 0) to predicted ones.\n",
    "    # testing on ch4, go up to 2014\n",
    "    #returns dataframe with datetime as index, and values for each day given\n",
    "    def year_to_days(self,df, colValName):\n",
    "        dateObjs = []\n",
    "        dateVals = []\n",
    "        for i, y in enumerate(df.year):\n",
    "            if y >= 1960:\n",
    "                start = dt.date(y, 1, 1)\n",
    "                end = dt.date(y+1,1, 1)\n",
    "                date = start\n",
    "                days = (end - start).days\n",
    "                value = df[colValName][i]\n",
    "                while date != end:\n",
    "                    dateObjs.append(date)\n",
    "                    dateVals.append(value/days)\n",
    "                    date += dt.timedelta(days=1)\n",
    "        df = pd.DataFrame(dateVals, index=dateObjs, columns=[colValName])\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        self.dayDataFrame = df\n",
    "        self.yearDataFrame = pd.DataFrame(df[colValName].resample('Y').sum(), columns=[colValName])\n",
    "        self.monthDataFrame = pd.DataFrame(df[colValName].resample('M').sum(), columns=[colValName])\n",
    "        self.weekDataFrame = pd.DataFrame(df[colValName].resample('W').sum(), columns=[colValName])\n",
    "    def month_to_days(self, df, colValName):\n",
    "        dateObjs = []\n",
    "        dateVals = []\n",
    "        if int(df.year[0]) > 1960:\n",
    "\n",
    "            diff = int(df.year[0]) - 1960\n",
    "            # find delta, and append to month range list\n",
    "\n",
    "            for year in range(diff+1):\n",
    "                for month in range(1, 13):\n",
    "                    # check if current year needs months added\n",
    "                    if 1960+year == int(df.year[0]) and int(df.month[0]) == month:\n",
    "                        break\n",
    "                    for days in range(1, (monthrange(int(df.year[0]) + year, month)[1])):\n",
    "                        dateObjs.append(dt.datetime(1960+year, month, days))\n",
    "                        dateVals.append(np.nan)\n",
    "        # add known dates to monthrange\n",
    "        for index, row in df.iterrows():\n",
    "            # check if row and month exist in delta time, if it doesn't add it.\n",
    "\n",
    "            numDaysInMonth = (monthrange(int(row['year']), int(row['month']))[1])\n",
    "            valueToAppendToDay = int(row[colValName])/numDaysInMonth\n",
    "            for days in range(1, numDaysInMonth+1):\n",
    "                dateObjs.append(dt.datetime(int(row['year']), int(row['month']), days))\n",
    "                dateVals.append(valueToAppendToDay)\n",
    "\n",
    "        # TODO HERE:  add dates after the end date of the given df to equal 2019.... maybe..\n",
    "        df = pd.DataFrame(dateVals, index=dateObjs, columns=[colValName])\n",
    "        Y = regrade_lin([x for x in range(len(df[colValName].values.tolist()))],df[colValName].values.tolist())\n",
    "        for val in Y:\n",
    "            if val > 0:\n",
    "                try:\n",
    "                    foo = 1/val\n",
    "                except ZeroDivisionError:\n",
    "                    continue\n",
    "                set = val\n",
    "                break\n",
    "        for i, v in enumerate(Y):\n",
    "            if v < 0.0000001 or v == 0:\n",
    "                Y[i] = set\n",
    "        df[colValName] = Y\n",
    "        self.dayDataFrame = df\n",
    "        self.yearDataFrame = pd.DataFrame(df[colValName].resample('Y').sum(), columns=[colValName])\n",
    "        self.monthDataFrame = pd.DataFrame(df[colValName].resample('M').sum(), columns=[colValName])\n",
    "        self.weekDataFrame = pd.DataFrame(df[colValName].resample('W').sum(), columns=[colValName])\n",
    "\n",
    "    def graphMonths(self, name):\n",
    "        plt.plot(self.monthDataFrame)\n",
    "        plt.savefig('{}_month_graph'.format(name))\n",
    "        plt.figure().clear()\n",
    "    def graphWeeks(self,name):\n",
    "        plt.plot(self.weekDataFrame)\n",
    "        plt.savefig('{}_weeks_graph'.format(name))\n",
    "        plt.figure().clear()\n",
    "    def graphDays(self,name):\n",
    "        plt.plot(self.dayDataFrame)\n",
    "        plt.savefig('{}_days_graph'.format(name))\n",
    "        plt.figure().clear()\n",
    "\n",
    "def regrade_lin(x, y):#returns the missing values of y\n",
    "    missing = []\n",
    "    n = 0\n",
    "    sumx = 0\n",
    "    sumy = 0\n",
    "    sum_prodxy = 0\n",
    "    sum_squarex = 0\n",
    "    sum_squarey = 0\n",
    "    for i,v in enumerate(y):\n",
    "        if pd.isna(v) or pd.isna(x[i]):\n",
    "            missing.append(i)\n",
    "        if not pd.isna(v) and not pd.isna(x[i]):\n",
    "            n+=1\n",
    "            sumx += x[i]\n",
    "            sumy += v\n",
    "            sum_prodxy += x[i]*v\n",
    "            sum_squarex += x[i]**2\n",
    "            sum_squarey += v**2\n",
    "    #some method from the internet\n",
    "    #a = (sumy*sum_squarex - sumx*sum_prodxy)/(n*sum_squarex - sumx**2)\n",
    "    #b = (n*sum_prodxy - sumx*sumy)/(n*sum_squarex - sumx**2)\n",
    "    #method of least squares\n",
    "    b = (sum_prodxy-(sumx*sumy)/n)/(sum_squarex-(sumx**2)/n)#b1\n",
    "    a = (1/n)*(sumy - b*sumx)#b0\n",
    "    #y = a + bx\n",
    "    #x = (y - a)/b\n",
    "    for i in missing:\n",
    "        if pd.isna(x[i]):\n",
    "            x[i] = (y[i] - a)/b\n",
    "        else:\n",
    "            y[i] = a + b*x[i]\n",
    "    return y\n",
    "\n",
    "def IPA(df):#value Increase in Percentage Averaged over intervals\n",
    "    ratios = []\n",
    "    for i,new in enumerate(df):\n",
    "        if i != 0:\n",
    "            ratios.append(((new-old)/old)*100)\n",
    "        old = new\n",
    "    print(len(ratios))\n",
    "    avg = sum(ratios)/len(ratios)\n",
    "    return ratios\n",
    "# if called from main, we want to test this file, so create dataframes and pass em in.\n",
    "# TODO: might want to put graph_all, and graph_weekly into new class, along with our training models.\n",
    "def test_code(debug):\n",
    "    sf6_data = pd.read_csv('./data/Sf6/sf6_mm_gl.csv', header=0)\n",
    "    sf6_obj = DateMod(sf6_data, 'average')\n",
    "    n2o_data = pd.read_csv('./data/N2o/n2o_mm_gl.csv',header=0)\n",
    "    n2o_obj = DateMod(n2o_data, 'average')\n",
    "\n",
    "\n",
    "    listOfCh4 = []\n",
    "    i = 0\n",
    "    for file in glob.glob('./data/Ch4/*'):\n",
    "        ch4_data = pd.read_csv(file, header=0)\n",
    "        listOfCh4.append(DateMod(ch4_data, 'value'))\n",
    "\n",
    "        if i==10:\n",
    "            break\n",
    "        i+=1\n",
    "    joinedCH4 = pd.DataFrame()\n",
    "    # concat all dataframes we made, and create sum\n",
    "    for ch4DataFrame in listOfCh4:\n",
    "        joinedCH4 = pd.concat([joinedCH4, ch4DataFrame.monthDataFrame], axis=1)\n",
    "\n",
    "    # aggregate each column to get sum of all of the files we scraped.\n",
    "    joinedCH4 = joinedCH4.agg('sum',axis=1)\n",
    "\n",
    "\n",
    "    # Don't know how to do these conversion.  Need help.\n",
    "    co2_data = pd.read_csv('./data/CO2Emission/global.1751_2014.csv', header=0)\n",
    "\n",
    "\n",
    "def usage():\n",
    "    print('python DataModifier [-d]')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
